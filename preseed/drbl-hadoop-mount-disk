#! /bin/sh
### BEGIN INIT INFO
# Provides:          drbl-hadoop-mount-disk
# Required-Start:    $network $local_fs
# Required-Stop:
# Should-Start:      $named
# Should-Stop:
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description: mount local disk for Hadoop datanode daemon
### END INIT INFO

# The basic layout for hadoop datanode in each disk. The first partition is swap 
# and second is data partition with unix  filesystem for drbl hardoop. eg:
# |---------|----------------------------|
# | SWAP 1G | second part for data node  |
# |---------|----------------------------|
# go...

#TODO
# add lvm support
# add multi disk support

set -e

prompt_for_exist_partition="N" # don't touch exist data and partition table
#prompt_for_exist_partition="y" # check exist data and ask to re-create new partition or not
# or rcho "y" | ./haduzilla-auto-part to always re-create partition table


# define variables
UUID="e367716d-0646-4507-b630-d8148c3f2a13"
DEFAULT_DISK="/dev/sda"

#
DEFAULT_FS="ext4"
DEFAULT_FS_OPTS="-U $UUID"
#
#DEFAULT_FS="xfs"
#DEFAULT_FS_OPTS="-f " #add xfs_admin -U $UUID /dev/sda
#
PART=""
SWAP=""
HADOOP_MOUTPOINT="/var/lib/hadoop-0.20/"
HADOOP_USER="root"
HADOOP_GROUP="hadoop"
HADOOP_DATANODE_USER="hdfs"
HADOOP_TASKTRACKER_USER="mapred"

# load custom variable to change DEFAULT_DISK
if [ -f /etc/default/drbl-hadoop ] ; then
  # Loading user defined DISK and HADOOP_MOUNTPOINT for custom reason
  . /etc/default/drbl-hadoop
fi

# load hadoop env variable
if [ -f /etc/default/hadoop-0.20 ] ; then
  . /etc/default/hadoop-0.20
  HADOOP_MOUNTPOINT=$(find /etc/hadoop/conf/ -name core-site.xml -exec grep -A1 hadoop.tmp.dir '{}'  \; | grep value | sed -e s#\<value\>## -e s#cache/\${user.name}\</value\>## -e s/\ //g)
fi

PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin
NAME=drbl-hadoop-mount-disk
DESC="mount local disk for Hadoop datanode daemon"

PIDFILE=/var/run/drbl-hadoop.pid
DODTIME=3                   # Time to wait for the server to die, in seconds
                            # If this value is set too low you might not
                            # let some servers to die gracefully and
                            # 'restart' will not work

# define functions
erase_pt(){
    DISK=$1
    echo "clean MBR"
    dd if=/dev/zero of=$DISK bs=512 count=32
}

create_pt(){
    parted -s $DISK  mklabel gpt
    parted -s $DISK  mkpart primary linux-swap 2048s 1955839s
    parted -s $DISK  mkpart primary 1955840s 100%
}

new_swap(){
    DISK=$1
    SWAP=$(echo "$DISK"1)
    mkswap $SWAP
}

new_partition(){
# test the speed of mkfs.EFAULT_FS and partclone.EFAULT_FS
    PART=$(echo "$DISK"2)
    mkfs.$DEFAULT_FS $DEFAULT_FS_OPTS $PART
    sync
}

create_parts(){
	erase_pt $DEFAULT_DISK
        create_pt $DEFAULT_DISK
	new_swap $DEFAULT_DISK
        new_partition $DEFAULT_DISK
}

check_UUID(){
    # test is empty disk or not
    set +e
    parts=$(blkid)
    test_uuid=$(echo $parts | grep $(echo $DEFAULT_DISK)2 | grep $UUID)
    if [ -z "$test_uuid" ]; then
        echo "fail"
    else
        echo "ok"
    fi
    set -e
}

check_disk_layout(){

    ret=$(check_UUID)

    # is empty, let's do it
    if [ X"$ret" != X"ok" ]; then
	create_parts
    else
    # not empty, confirm really erase and create partition for hadoop
    	if [ X"$prompt_for_exist_partition" != X"N" ]; then
            echo "erase data and create new partitions"
	    echo -n "[y/N] "
	    read ans
	    case "$ans" in y|Y) 
	    # say yes, let's do it
	        echo "let's do it"
	        create_parts
	        ;;
	    *) 
	    # say no, to skip
	        echo "exit" 
	        exit 1
	        ;;
	    esac
        fi
    fi
    # done, and re-reading partition table
    sleep $DODTIME
    hdparm -z /dev/sda

    if [ -z "$PART" -o -z "$SWAP"  ]; then
	    PART=$(blkid | grep $DEFAULT_FS | awk {'print $1'} | sed s/://g)
	    SWAP=$(blkid | grep swap | awk {'print $1'} | sed s/://g)
    fi
}

prepare_folders(){
    mapred_dir=$HADOOP_MOUTPOINT/cache/mapred/mapred/local
    hdfs_dir=$HADOOP_MOUTPOINT/cache/hdfs
    mkdir -p $mapred_dir $hdfs_dir
    chown $HADOOP_USER:$HADOOP_GROUP $HADOOP_MOUNTPOINT
    chown $HADOOP_TASKTRACKER_USER:$HADOOP_TASKTRACKER_USER $mapred_dir
    chown $HADOOP_DATANODE_USER:$HADOOP_DATANODE_USER $hdfs_dir
}

restart_hadoop_services(){
    set +e
    data_node_status=$($(ls /etc/init.d/hadoop*datanode) status | grep 'not running')
    if [ -z "$data_node_status" ]; then
        $(ls /etc/init.d/hadoop*datanode) restart
        $(ls /etc/init.d/hadoop*tasktracker) restart
    fi
    set -e
}

start() {
  check_disk_layout

  if [ ! -x $HADOOP_MOUNTPOINT ]; then
    mkdir -p $HADOOP_MOUNTPOINT
  fi

  if [ -z "$PART"  ]; then
    echo "something wrong, can't found data node partition"
    exit 2
  fi

  if [ -z "$SWAP"  ]; then
    echo "something wrong, can't found swap partition"
    exit 2
  fi
  cat /proc/partitions
  mount $PART $HADOOP_MOUNTPOINT
  prepare_folders
  swapon $SWAP
  restart_hadoop_services
  touch $PIDFILE
}

stop() {
  SWAP=$(blkid | grep swap | awk {'print $1'} | sed s/://g)
  umount $HADOOP_MOUNTPOINT
  swapoff $SWAP
  restart_hadoop_services
  rm -f $PIDFILE
}


case "$1" in
  start)
        echo -n "Starting $DESC: "
        start
        if [ -f $PIDFILE ] ; then
            echo "OK."
        else
            echo "ERROR."
        fi
  ;;
  stop)
        echo -n "Stopping $DESC: "
        stop
        if [ ! -f $PIDFILE ] ; then
            echo "OK."
        else
            echo "ERROR."
        fi
  ;;
  force-reload)
	if [ -f $PIDFILE ] ; then
	  $0 restart
	fi
  ;;
  restart)
        echo -n "Restarting $DESC: "
        if [ -f $PIDFILE ] ; then 
	  stop 
	fi
        [ -n "$DODTIME" ] && sleep $DODTIME
        $0 start
  ;;
  status)
    echo -n "$PART is "
    if [ -f $PIDFILE ] ;  then
        echo "mounted"
    else
        echo "not mounted."
        exit 1
    fi
    ;;
  *)
  N=/etc/init.d/$NAME
  echo "Usage: $N {start|stop|restart|force-reload|status}" >&2
  exit 1
  ;;
esac

exit 0
